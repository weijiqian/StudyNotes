java面试要点

书写规则

### 核心技术

​	 1 java基础扎实，掌握jvm原理，多线程，网络原理，设计模式，常用数据结构和算法。
​	 2 深入理解spring，spring mvc ，mybatist等开源框架设计及底层架构，研究过核心功能源码，具备一定的框架定制开发能力
​	 3 深入理解Redis线程模型，熟练掌握redis的和核心数据结构的使用场景，熟悉多级缓存架构，熟悉各种缓存搞并发的使用场景，比如缓存雪崩，缓存穿透，缓存失效，热点缓存重建等。
​	4 熟悉常见消息中间件的使用，解决过各种消息通信场景的疑难问题，比如消息丢失，消息重复消费，消息顺序性，大规模消息积压问题。
​	5 对于高性能IO通信模型以及相关开源组件Netty等源码有过深度研究，熟悉Netty线程模型，熟悉百万级并发服务器架构设计。
​	6 深入理解JVM底层原理，熟悉JVM各种垃圾收集器的是使用以及核心参数的调优，有过一定的JVM线上调优经验，对JVM调优有自己独到的见解。
​	7 深入理解spring  boot ，spring cloud，dubbo等微服务框架的设计原理及底层架构，研究过核心源码，熟悉各种微服务架构场景设计，比如服务注册与发现,服务限流，降级，熔断，服务网关路由设计，服务安全认证架构。
​	 8 在项目中解决过各种分布式场景的技术难题，比如分布式锁，分布式事务，分布式session，分布式任务，还力量数据的分库分表。





### 大数据技能描述



#### 案例1

1．能搭使用Hive和HBase的系统架构，和能用Hive进行海量数据的统计分析以及，能根据需求设计HBase表，能对Hive、HBase进行搭建Hadoop的系统架构和Hadoop集群。 

2.能使用开源日志收集框架flume，Kafka消息队列。

 3.能够使用Python2.7的版本，进行编程部署， 实现MapReducer框架，解决离线分析的场景和作业。

 4.能够使用java,Scala进行项目的开发，能够实现Spark框架的研发作业。

 5.能使用SparkStreaming进行实时数据的分析。

 6.能使用MapReduce的原理和流程和其API开发应用程序 

7．能使用Storm的原理和流程和使用API开发应用程序，以及Storm+Kafka实时流处理架构

 8.清楚spark任务提交流程，对spark作业能进行调优 

9.能操作MySQL、Oracle数据库

 10.能使用Python进行网页爬虫，和Python与MySQL的交互



#### 案例2

对于Linux的基本常用命令比较熟练，可以熟练使用vi编辑器。
熟悉zookeeper分布式协调服务应用，了解zookeeper的运行原理；熟练运用spark，清楚spark的启动过程以及任务运行的基本流程，可以自主完成对spark集群的搭建，熟悉spark-streaming，spark-sql，对于spark的相关算法有一些了解。
熟练掌握scala语言，可以使用scala对spark程序进行一些编写；
熟悉python语言，可以使用python来进行对业务逻辑的处理，以及爬取数据等操作；
熟悉Kafka的工作原理，可以自主完成对Kafka集群的搭建；
熟练运用shell脚本进行编程，可以使用shell脚本来进行一些特定的操作
熟悉hdfs，mapreduce，yarn的工作原理，熟悉Hadoop的生态体系，可以独立完成对于Hadoop集群的搭建，同时对于mapreduce程序的开发也可以独立完成；
熟悉hbase，hive，熟悉flume与Kafka的综合使用。
可以熟练使用eclipse，myeclipse，idea，mysql，等相关开发工具